{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import copy\n",
    "import astropy\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdul = fits.open('http://gal-03.sai.msu.ru/~vtoptun/photometry/rcsed_v2_clean.fits', memmap=astropy.io.fits.Conf.use_memmap.defaultvalue, lazy_load_hdus=True)\n",
    "# hdul = fits.open('http://gal-03.sai.msu.ru/~vtoptun/redshift/rcsed_v2.fits')\n",
    "hdul = fits.open('rcsed_v2_clean.fits', memmap=astropy.io.fits.Conf.use_memmap.defaultvalue, lazy_load_hdus=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = hdul[1].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hdul[1].data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdul.close()\n",
    "del hdul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.DataFrame(np.array(data).byteswap().newbyteorder()) #[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.info(verbose=True,null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = DATA.select_dtypes(exclude='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_float = DATA.select_dtypes(include=['float32', 'float64'])\n",
    "data_int = DATA.select_dtypes(include=['int8','int16','int32','int64','uint8'])\n",
    "\n",
    "converted_float = data_float.apply(pd.to_numeric,downcast='float')\n",
    "converted_int = data_int.apply(pd.to_numeric,downcast='unsigned')\n",
    "\n",
    "del data_float, data_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.concat([converted_float, converted_int],axis=1)\n",
    "\n",
    "del converted_float,converted_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(data = [[1,2,3],[1,3,3]])\n",
    "# df.replace([3,2],np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_list = [-2147483648, '', '', '', '', '', -9223372036854775808, -32768, -2147483648, -32768, '', -2147483648, '', '', -32768, '', -2147483648, -2147483648, '', '', '', '', '', '', '', -2147483648, -2147483648, -2147483648, '', 255, 255, '', -32768, -32768, '', '', -2147483648, '', -2147483648, '', -32768, '', '', -32768, '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(null_list).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.replace([-2147483648, -9223372036854775808,-32768,255, -999999488.0,\n",
    " 99.0,\n",
    " 0.0,\n",
    " -99.0,\n",
    " 9999.0,\n",
    " -999.0,\n",
    " 0.05299999937415123,\n",
    " float('inf'),\n",
    " 3.5,\n",
    " -9999.0,\n",
    " 4.619999885559082,\n",
    " 0.05000000074505806,\n",
    " 70.0,\n",
    " 4.0,\n",
    " 179.96949768066406,\n",
    " 0.06128999963402748,\n",
    " 24.6346492767334], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.dropna(axis=1,\n",
    "    how='all',\n",
    "    thresh=None,\n",
    "    subset=None,\n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (~DATA.z_sdss.isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.info(verbose=True,null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "DATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдения:**\n",
    "- в некоторых колонках,  в которых присутствуют все значния есть очень странные значения вроде \"-2147483648\" или \"-32768\" и другие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Здесь мы создадим датафрейм с наиболее \n",
    "#       частотными значениями по каждому из столбцов\n",
    "\n",
    "top_values = []\n",
    "\n",
    "for col in DATA.columns[1:]:\n",
    "    vc = DATA[col].value_counts(ascending=False).head(1)\n",
    "    top_values.append([vc.name, vc.index[0], vc[vc.index[0]]])\n",
    "    \n",
    "anomaly = pd.DataFrame(data=top_values, columns = ['column_name', 'anomaly_value', 'value_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь мы создаем колонку с количеством\n",
    "#  ненулевых значений в каждом из столбцов таблицы\n",
    "#     DATA, начиная с ra\n",
    "\n",
    "not_nan = []\n",
    "\n",
    "for col in DATA.columns[1:]:\n",
    "    not_nan.append((~DATA[col].isna()).sum())\n",
    "    \n",
    "not_nan = pd.Series(not_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дополним получившийся датафрейм столбцом, показывающим,\n",
    "# какую долю от длины всего столбца занимает каждое аномальное значение\n",
    "\n",
    "import copy\n",
    "\n",
    "top_values_data = copy.copy(anomaly)\n",
    "\n",
    "top_values_data['percent_total'] = (top_values_data.value_counts / len(DATA)).round(3)\n",
    "top_values_data['percent_not_nan'] = (top_values_data.value_counts / not_nan).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выведем на экран получившйся датафрейм\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(top_values_data.sort_values(by='percent_not_nan', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ данных на основе таблицы top_values_data\n",
    "\n",
    "Таблица **top_values_data** содержит в себе следующие столбцы:\n",
    "- **column_name**   -   названия столбцов таблицы **DATA**\n",
    "\n",
    "- **anomaly_value**   -   наиболее частотные значения столбцов таблицы **DATA**\n",
    "\n",
    "- **value_counts**   -   количество раз, которое *anomaly_value* встречается в столбце таблицы **DATA**\n",
    "\n",
    "- **percent_total**   -   доля *anomaly_value* от всего объема столбца таблицы **DATA**\n",
    "\n",
    "- **percent_not_nan**   -   доля *anomaly_value* от non-null объема столбца таблицы **DATA**\n",
    "\n",
    "В ячейке ниже показано, сколько столбцов таблицы **DATA** содержат более **90% и т.д.** аномальных значений от общего количества non-null значений столбца. Оказывается, что около трети столбцов (162) содержат таки аномалий больше, чем **10%**.\n",
    "\n",
    "**! Нужно понять, какой порог считать допустимым.**\n",
    "\n",
    "**! Остальные можно отбросить.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in np.arange(0.9,0.0,-0.05).round(2):\n",
    "    print(num, '   ', len(top_values_data.query('percent_not_nan > @num')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Добавим столбец non_null_part, показываающий \n",
    "#     какую часть от величины столбца составляют ненулевые значения\n",
    "\n",
    "top_values_data['non_null_part'] = (not_nan / len(DATA)).round(3)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(top_values_data.sort_values(by='non_null_part', ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выясним сколько столбцов таблицы DATA  заполнены меньше чем на num*100 %\n",
    "\n",
    "for num in np.arange(0.9,0.0,-0.1).round(2):\n",
    "    print(num, '   ', len(top_values_data.query('non_null_part < @num')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_values_data.query('(non_null_part<0.01)&(percent_not_nan<0.5)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выведем на экран получившйся датафрейм\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(top_values_data.sort_values(by='value_counts', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     display(DATA.describe(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(DATA.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_values_data[top_values_data['anomaly_value']==float('inf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly_to_drop = list(top_values_data.anomaly_value.value_counts()\n",
    "#                        [top_values_data.anomaly_value.value_counts(ascending=False)>1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA.replace(anomaly_to_drop, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(top_values_data.anomaly_value.value_counts()\n",
    "#                        [top_values_data.anomaly_value.value_counts(ascending=False)>1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Найдем колонки, в которых аномальные значения\n",
    "#     составляют не менее 90% от всех ненулевых значений столбца \n",
    "\n",
    "# percent_not_nan - доля anomaly_value от non-null объема столбца таблицы DATA\n",
    "# non_null_part - доля ненулевых значений  от величины столбца \n",
    "\n",
    "columns_drop = top_values_data[(top_values_data['percent_not_nan']*top_values_data['non_null_part'] > 0.9)\n",
    "                               |(top_values_data['non_null_part'] > 0.9)\n",
    "                               |(top_values_data['anomaly_value']==float('inf'))].column_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим колонки из DATA, в которых анамальные значения\n",
    "#     составляют о не менее 90% от всех ненулевых значений столбца \n",
    "\n",
    "DATA.drop(labels=columns_drop, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что мы очистили таблицу от всех столбцов, несущих мало информации, так как они содержали большое количество NaN или аномальных значений.\n",
    "\n",
    "**Следующим нашим шагом** будет использование **IterativeImputer** чтобы заполнить оставшиеся пустые значения таблицы.\n",
    "\n",
    "К новой заполненной таблице можно будет применить методы понижения разверности: VAE, t-SNE, PCA и др.\n",
    "\n",
    "А далее уже кластеризировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = DATA.select_dtypes(include=['float32', 'float64']).apply(pd.to_numeric,downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_columns = DATA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler #StandardScaler\n",
    "nrmlzr = MinMaxScaler()\n",
    "x = nrmlzr.fit_transform(DATA)\n",
    "del DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer = IterativeImputer(max_iter=10, tol=3*x.std().max(),random_state=42, \n",
    "#                            estimator=ExtraTreesRegressor(n_estimators=10, random_state=0))\n",
    "\n",
    "# imputer = IterativeImputer(max_iter=10, tol=3*x.std().max(),random_state=42, \n",
    "#                            estimator= DecisionTreeRegressor(max_features='sqrt', random_state=0))\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, tol=3*x.std().max(),random_state=42, \n",
    "                           estimator=KNeighborsRegressor(n_neighbors=5, n_jobs=20))\n",
    "\n",
    "x = imputer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y = pd.read_csv('rcsed_iGrID.csv')[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_indx = list(y[~y.iGrID.isna()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tsvd = TruncatedSVD(n_components=1, algorithm='randomized', n_iter=5, random_state=42)\n",
    "x_new = tsvd.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=1, allow_single_cluster=True).fit(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.loc[sdss_indx,:].to_numpy().flatten()\n",
    "# clusterer.labels_[sdss_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = sklearn.metrics.homogeneity_completeness_v_measure(y,clusterer.labels_[sdss_indx])\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_file = open(\"m1.txt\", \"w\")\n",
    "# n = text_file.write(str(m1))\n",
    "# text_file.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(0.9314625591121057, 0.9054237857387012, 0.9182586161732728)  BayesianRidge\n",
    "\n",
    "(0.9101561056302607, 0.902252573027413, 0.9061871065081326)   ExtraTreesRegressor(n_estimators=10, random_state=0)\n",
    "\n",
    "(0.9043550925918418, 0.9016872117005851, 0.9030191816529328)  DecisionTreeRegressor(max_features='sqrt', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA = pd.DataFrame(data=DATA_data, columns=DATA_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA.info(verbose=True,null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее применим VAE  (https://github.com/dgradoboev/project_galaxies/blob/master/VAE.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import sys\n",
    "# sys.path\n",
    "\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import astropy\n",
    "from astropy.coordinates import solar_system_ephemeris, EarthLocation\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import constants as const\n",
    "import hdbscan\n",
    "from astropy.io import fits\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "# from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return torch.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, x.shape[1]))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(x_dim=x.shape[1], h_dim1= int(x.shape[1]/2), h_dim2=int(x.shape[1]/4), z_dim=1)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=x, batch_size=int(len(x)/10), shuffle=True)\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    BCE = criterion(recon_x, x.view(-1, x.shape[1]))\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx in range(400):\n",
    "        #data = torch.tensor(x.sample(n=1000).astype(np.float32).values)\n",
    "        data = torch.tensor(x[np.random.choice(range(x.shape[0]), 1000)].astype(np.float32))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 70):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_var = vae.encoder(torch.tensor(x.astype(np.float32)))\n",
    "embed = vae.sampling(mu, log_var).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbScan = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=1, allow_single_cluster=True).fit(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = homogeneity_completeness_v_measure(y,hdbScan.labels_[sdss_indx])\n",
    "m2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(0.9473008374237644, 0.9062005475990196, 0.9262950052814476) BayesianRidge\n",
    "\n",
    "(0.9495008027780173, 0.9054467727534533, 0.9269506577358073) ExtraTreesRegressor(n_estimators=10, random_state=0)\n",
    "\n",
    "(0.9611316948686983, 0.9080748825406928, 0.933850288643485)  DecisionTreeRegressor(max_features='sqrt', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_file = open(\"m2.txt\", \"w\")\n",
    "# m = text_file.write(str(m2))\n",
    "# text_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
